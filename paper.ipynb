{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.[Projection Convolutional Neural Networks for 1-bit CNNs via Discrete Back Propagation](https://arxiv.org/abs/1811.12755)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 深度卷积神经网络（DCNN）的进步已经大大提高了许多计算机视觉任务的识别系统的准确性。 但是，它们的实际应用通常受限于资源受限的环境。 在本文中，我们引入投影卷积神经网络（PCNN）与离散反向传播投影（DBPP），以提高二值化神经网络（BNNs）的性能。 本文的贡献包括：1）首次利用投影函数有效地解决离散反向传播问题，从而产生新的高度压缩的CNN（称为PCNN）; 2）通过利用多个投影，我们学习了一组不同的量化内核，这些内核以比以前提出的更有效的方式压缩全精度内核; 3）与ImageNet和CIFAR数据集上的其他最先进的BNN相比，PCNN实现了最佳的分类性能。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.[Projection Convolutional Neural Networks for 1-bit CNNs via Discrete Back Propagation](https://pdfs.semanticscholar.org/57d4/19a81551e9c2efa20b87aaa9387c82c08607.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 在用于分类的大多数现有深度卷积神经网络（CNN）中，全局平均（一阶）汇集（GAP）已成为对最后卷积层的激活进行求和的标准模块，作为预测的最终表示。最近的研究表明集成高阶汇集（HOP）方法明显改善了深CNN的性能。然而，GAP和现有的HOP方法都假定单峰分布，这不能完全捕获卷积激活的统计数据，限制深CNN的表示能力，特别是对于具有复杂内容的样本。为克服上述局限，本文提出了一种全局门控二阶池（GM-SOP）混合方法，进一步提高了深层CNN的表示能力。为此，我们引入了稀疏约束门控机制，并提出了一种新的参数SOP作为混合模型的组成部分。给定一组SOP候选者，我们的方法可以通过稀疏度约束的门控模块自适应地为每个输入样本选择Top-K（K> 1）候选者，并且执行Kselected候选者的输出的加权和作为样本的表示。拟议的GM-SOP可以在高效的路径中灵活地容纳大量的个性化SOP候选人，从而实现更丰富的表现形式。我们的GM-SOP的深层网络可以进行端到端的训练，有可能表征复杂的多模式分布。所提出的方法在两个大规模图像基准（即，下采样的ImageNet-1K和Places365）上进行评估，并且实验结果表明我们的GM-SOP优于其对应物并且实现了非常有竞争力的性能。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.[Independently recurrent neural network (indrnn): Building A longer and deeper RNN](https://arxiv.org/abs/1803.04831)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.[Network Structure and Transfer Behaviors Embedding via Deep Prediction Model](https://www.aaai.org/Papers/AAAI/2019/AAAI-SunXin.7117.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.[Self-Supervised Convolutional Subspace Clustering Network](http://www.cis.pku.edu.cn/faculty/vision/zlin/Publications/2019-CVPR-S2ConvSCN.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.none"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.[ComDefend: An Efficient Image Compression Model to Defend Adversarial Examples](https://arxiv.org/abs/1811.12673)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 已经证明深度神经网络（DNN）容易受到对抗性的影响。具体而言，为清洁图像添加难以察觉的扰动可能会欺骗训练有素的深度神经网络。在本文中，我们提出了一种端到端的图像压缩模型来防御对抗性的例子：\\ textbf {ComDefend}。所提出的模型由压缩卷积神经网络（ComCNN）和重建卷积神经网络（ResCNN）组成。 ComCNN用于维护原始图像的结构信息并净化对抗性扰动。 ResCNN用于重建高质量的原始图像。换句话说，ComDefend可以将对抗图像转换为其干净版本，然后将其提供给训练有素的分类器。我们的方法是一个预处理模块，并不会在整个过程中修改分类器的结构。因此，它可以与其他模型特定的防御模型相结合，共同提高分类器的鲁棒性。在MNIST，CIFAR10和ImageNet上进行的一系列实验表明，所提出的方法优于最先进的防御方法，并且始终有效地保护分类器免受敌对攻击。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.none"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.[Understanding Humans in Crowded Scenes: Deep Nested Adversarial Learning and A New Benchmark for Multi-Human Parsing](https://arxiv.org/abs/1804.03287)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 尽管在诸如检测，实例分割和人工解析等感知任务方面取得了显着进步，但计算机在拥挤的场景中仍然无法令人满意地理解人类，例如群体行为分析，人员重新识别和自动驾驶等。为此，模型需要全面感知多人图像中的语义信息和实例之间的差异，最近将其定义为多人解析任务。在本文中，我们提出了一个新的大型数据库“多人解析（MHP）”用于算法开发和评估，并推进了在拥挤场景中理解人类的最新技术。 MHP包含25,403个精心注释的图像，具有58个细粒度语义类别标签，每个图像涉及2-26个人，并且在各种视点，姿势，遮挡，交互和背景的真实场景中捕获。我们进一步提出了一种用于多人解析的新型深嵌套对抗网络（NAN）模型。 NAN由三个类似生成对抗网络（GAN）的子网组成，分别执行语义显着性预测，实例不可知解析和实例感知聚类。这些子网形成一个嵌套结构，经过精心设计，以端到端的方式共同学习。 NAN在我们的MHP和其他几个数据集上始终优于现有最先进的解决方案，并作为推动未来多人解析研究的强大基线。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.[FishNet: A Versatile Backbone for Image, Region, and Pixel Level Prediction](https://arxiv.org/abs/1901.03495)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 设计用于预测不同级别（例如，图像级，区域级和像素级）上的对象的卷积神经网络（CNN）结构的基本原理是不同的。通常，专门用于图像分类的网络结构直接用作包括检测和分段在内的其他任务的默认骨干结构，但是在考虑统一为像素级或区域级预测设计的网络的优点的考虑下，很少设计骨干结构。任务，可能需要具有高分辨率的非常深的功能。为实现这一目标，我们设计了一个名为FishNet的鱼类网络。在FishNet中，所有决议的信息都会保留并针对最终任务进行细化。此外，我们观察到现有的作品仍然无法“直接”将梯度信息从深层传播到浅层。我们的设计可以更好地解决这个问题已经进行了大量实验来证明FishNet的卓越性能。特别是在ImageNet-1k上，FishNet的精度能够以更少的参数超越DenseNet和ResNet的性能。 FishNet被应用为COCO Detection 2018挑战赛获奖作品中的一个模块。此代码可通过此https URL获得。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.[BiSeNet: Bilateral Segmentation Network for Real-time Semantic Segmentation](https://arxiv.org/abs/1808.00897)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 语义分割需要丰富的空间信息和相当大的感受野。然而，现代方法通常会牺牲空间分辨率来实现实时推理速度，从而导致性能不佳。在本文中，我们通过一种新颖的双边分割网络（BiSeNet）来解决这一难题。我们首先设计一个具有小步幅的空间路径，以保留空间信息并生成高分辨率特征。同时，采用具有快速下采样策略的上下文路径来获得足够的感知域。在这两条路径的顶部，我们引入了一个新的功能融合模块，以有效地结合功能。所提出的架构在Cityscapes，CamVid和COCO-Stuff数据集上的速度和分段性能之间取得了适当的平衡。具体来说，对于2048x1024输入，我们在Cityscapes测试数据集上实现了68.4％的平均IOU，在一块NVIDIA Titan XP卡上的速度为105 FPS，这明显快于具有可比性能的现有方法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.[Dual Attention Network for Scene Segmentation](https://arxiv.org/abs/1809.02983)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 在本文中，我们通过基于自我约束机制捕获丰富的上下文依赖关系来解决场景分割任务。与之前通过多尺度特征融合捕获上下文的工作不同，我们提出了一种双重注意网络（DANet）来自适应地集成本地特征及其全局依赖性。具体来说，我们在传统的扩张FCN之上附加两种类型的注意模块，它们分别对空间和通道维度中的语义相互依赖性进行建模。位置关注模块通过所有位置处的特征的加权和来选择性地聚合每个位置处的特征。无论距离如何，相似的特征都将彼此相关。同时，信道关注模块通过整合所有信道映射中的相关特征来选择性地强调相互依赖的信道映射。我们将两个注意模块的输出相加以进一步改进特征表示，这有助于更精确的分割结果。我们在三个具有挑战性的场景分割数据集上实现了新的最先进的分割性能，即Cityscapes，PASCAL Context和COCO Stuff数据集。特别是，在不使用粗略数据的情况下，Cityscapes测试集的平均IoU得分为81.5％。我们通过此https网址公开提供代码和经过培训的模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13.[PartNet: A Recursive Part Decomposition Network for Fine-grained and Hierarchical Shape Segmentation](https://arxiv.org/abs/1903.00709)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3D形状分割的深度学习方法通​​常被表述为多类标签问题。现有模型针对固定标签集进行训练，这极大地限制了它们的灵活性和适应性。我们选择自上而下的递归分解，并基于递归神经网络开发第一个用于3D形状分层分割的深度学习模型。从表示为点云的完整形状开始，我们的模型执行递归二进制分解，其中层次结构中所有节点处的分解网络共享权重。在每个节点处，训练节点分类器以确定类型（邻接或对称）并停止其分解的标准。在较高级别节点中提取的特征被递归地传播到较低级别的节点。因此，较高级别的有意义分解提供了强大的上下文线索，从而限制了较低级别的分段。同时，为了提高每个节点的分割精度，我们利用为相应部分提取的形状特征来增强递归上下文特征。我们的方法根据形状复杂性将点云中的3D形状划分为不固定数量的部分，显示出强大的通用性和灵活性。它实现了最先进的性能，包括细粒度和语义分割，公共基准以及本工作中提出的细粒度分割的新基准。我们还展示了其在图像到形状重建中的细粒度零件细化的应用。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 14.[Attention-guided Unified Network for Panoptic Segmentation](https://arxiv.org/abs/1812.03904)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 本文研究全景分割，这是最近提出的任务，它在实例级别对前景（FG）对象进行分段，在语义级别对背景（BG）内容进行分段。 现有方法主要分别处理这两个问题，但在本文中，我们揭示了它们之间的潜在关系，特别是，FG对象提供了辅助线索以帮助BG理解。 我们的方法称为注意力引导统一网络（AUNet），是一个统一的框架，同时具有两个分支用于FG和BG分割。 BG分支中添加了两个注意事项，即RPN和FG分割掩码，分别提供对象级和像素级注意。 我们的方法推广到不同的骨干，在FG和BG分割中具有一致的准确度增益，并且还在MS-COCO（46.5％PQ）基准中设定了新的最新技术水平。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 15.[Elastic Boundary Projection for 3D Medical Imaging Segmentation](https://arxiv.org/abs/1812.00518)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "我们专注于一个重要且具有挑战性的问题：使用2D深度网络来处理医学成像分析的3D分割。为此目的，现有方法应用多视图平面（2D）网络或直接使用体积（3D）网络，但它们都不理想：2D网络无法有效捕获3D上下文，3D网络既耗费内存又少由于缺乏预先训练的模型，可以说是稳定的。\n",
    "在本文中，我们使用一种名为弹性边界投影（Elastic Boundary Projection，EBP）的新方法弥合了2D和3D之间的差距。关键的观察是，虽然物体是3D体积，但我们在分割中真正需要的是找到它的2D表面边界。因此，我们在3D空间中放置了许多枢轴点，并且对于每个枢轴，我们确定它沿着一组密集方向到达对象边界的距离。这在每个枢轴周围创建了一个弹性外壳，初始化为一个完美的球体。我们训练2D深度网络以确定每个终点是否落入对象内，并逐渐调整壳体以使其逐渐收敛到边界的实际形状，从而实现分割的目标。 EBP允许3D分割而无需将体积切割成切片或小块，这在传统的2D和3D方法中脱颖而出。 EBP在从CT扫描中分割几个腹部器官方面取得了很好的准确性。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "16.none"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 17.[User-Guided Deep Anime Line Art Colorization with Conditional Adversarial Networks](https://arxiv.org/abs/1808.03240)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "不相关"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 18.[Residual-Guide Feature Fusion Network for Single Image Deraining](https://arxiv.org/abs/1804.07493)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由于雨天图像对许多计算机视觉系统产生不利影响，因此单图像雨条去除非常重要。基于深度学习的方法在图像去除任务中取得了巨大成功。在本文中，我们提出了一种新的残差引导特征融合网络，称为ResGuideNet，用于单图像去除，逐步预测高质量重建。具体来说，我们提出了一个级联网络，并采用较浅的块生成的残差来引导更深的块。通过使用这种策略，我们可以在块越来越深时获得负剩余的粗略到精细估计。不同块的输出合并到最终重建中。我们采用递归卷积来构建每个块并对所有中间结果应用监督，这使我们的模型能够在合成和真实数据上实现有希望的性能，同时使用比先前所需更少的参数。 ResGuideNet可拆卸，以满足不同的雨天条件。对于在测试时具有小雨条纹和有限计算资源的图像，即使有几个构建块，我们也可以获得不错的性能。实验验证ResGuideNet可以使其他低级和高级视觉任务受益。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 19.[All-In-Focus Synthetic Aperture Imaging Using Image Matting](http://www0.cs.ucl.ac.uk/staff/R.Yu/papers/AllSAI.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "不相关"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 20.[Robust Single-Image Super-Resolution  Based on Adaptive Edge-Preserving  Smoothing Regularization](http://www.kresttechnology.com/krest-academic-projects/krest-mtech-projects/ECE/M-TECH%20DSPDIP%202018-19/M.Tech%20DSP%20BasePaper%202018-2019/[12].pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "摘要 - 通过稀疏表示的单图像超分辨率（SR）重建最近引起了广泛的兴趣。众所周知，低分辨率（LR）图像由于观察图像的劣化而易受噪声或模糊影响，这将导致较差的SR性能。在本文中，我们在稀疏表示的框架中提出了一种新的鲁棒边缘保持平滑超分辨率（REPS-SR）方法。基于梯度域引导滤波设计边缘保持平滑（EPS）正则化项，以保留图像边缘并减少重建图像中的噪声。此外，提出了通过估计LR图像的噪声水平而没有人工干扰而自适应地确定的平滑感知因子，以获得数据保真度项与所提出的EPS正则化项之间的最佳平衡。迭代收缩算法用于获得LR图像的SR图像结果。所提出的自适应平滑感知方案使得我们的方法对不同的噪声水平具有鲁棒性。实验结果表明，所提出的方法可以保留图像边缘并降低噪声，并且优于噪声图像的当前最新方法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 21.[CNN-based Real-time Dense Face Reconstruction with Inverse-rendered Photo-realistic Face Images]()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "随着卷积神经网络（CNN）的强大，基于CNN的面部重建最近在从2D面部图像重建详细的面部形状方面显示出有希望的性能。基于CNN的方法的成功依赖于大量标记数据。现有技术使用粗糙的可变形面部模型来合成这样的数据，然而该模型难以生成面部（具有皱纹）的详细照片般逼真的图像。本文提出了一种新颖的**人脸数据生成方法**。具体而言，我们基于逆渲染渲染具有不同属性的大量照片般逼真的面部图像。此外，我们通过将不同比例的细节从一个图像转移到另一个图像来构建精细的面部图像数据集。我们还通过模拟真实视频数据的分布来构造大量视频类型的相邻帧对。通过这些构建良好的数据集，我们提出了一个由三个卷积网络组成的粗到精学习框架。训练网络用于从单眼视频以及单个图像进行实时详细的3D面部重建。大量的实验结果表明，与现有技术相比，我们的框架可以产生高质量的重建，但计算时间要少得多。此外，由于数据的多样性，我们的方法对姿势，表达和照明具有鲁棒性。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 22.[Learning Warped Guidance for Blind Face Restoration](https://arxiv.org/abs/1804.04829)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本文研究了无约束模糊，噪声，低分辨率或压缩图像（即退化观察）的盲面恢复问题。为了更好地恢复精细的面部细节，我们通过将降级的观察结果和与我们的引导面部恢复网络（GFRNet）输入相同身份的高质量引导图像进行修改来修改问题设置。然而，降级的观察和引导图像通常在姿势，照明和表达方面不同，从而使得普通CNN（例如，U-Net）无法恢复精细和身份识别的面部细节。为了解决这个问题，我们的GFRNet模型包括一个变形子网（WarpNet）和一个重建子网（RecNet）。引入WarpNet来预测流动场以扭曲引导图像以校正姿势和表达（即，翘曲引导），而RecNet将退化观察和翘曲引导作为输入以产生恢复结果。由于地面实况流场不可用，因此引入了具有里程碑意义的损失以及总变差正则化来指导WarpNet的学习。此外，为了使该模型适用于盲目恢复，我们的GFRNet在合成数据上进行了训练，具有模糊内核，噪声水平，下采样比例因子和JPEG品质因数的多种设置。实验表明，我们的GFRNet不仅能够有效地对抗最先进的图像和面部修复方法，还可以在真实降级的面部图像上生成视觉上逼真的照片效果。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 23.[1FFDNet: Toward a Fast and Flexible Solution forCNN based Image Denoising](https://arxiv.org/pdf/1710.04026.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "摘要 - 由于快速推理和良好的性能，在图像去噪中广泛研究了判别学习方法。然而，这些方法主要学习每个噪声水平的特定模型，并且需要多个模型来对具有不同噪声水平的图像进行去噪。它们还缺乏处理空间变化噪声的灵活性，限制了实际去噪中的应用。为了解决这些问题，我们提出了一种快速灵活的去噪卷积神经网络，即FFDNet，它具有可调噪声水平图作为输入。所提出的FFDNet适用于下采样的子图像，在推理速度和去噪性能之间实现了良好的折衷。与现有的判别式消毒器相比，FFDNet具有多种理想特性，包括（i）能够有效处理各种噪声水平（即[0,75]），（ii）能够有效地消除变量噪声通过指定非均匀噪声水平图，以及（iii）速度比基准BM3D更快，即使在CPU上也不会牺牲去噪性能。与现有技术的降噪器相比，进行了合成和真实噪声图像的广泛实验以评估FFDNet。结果表明FFDNet是有效和高效的，使其在实际去噪应用中具有很强的吸引力"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 24.[Shift-Net: Image Inpainting via Deep Feature Rearrangement](https://arxiv.org/abs/1801.09392)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "深度卷积网络（CNN）已经展示了其在**图像修复**中产生合理结果的潜力。然而，在大多数现有方法中，例如，上下文编码器，通过将周围的卷积特征传播通过完全连接的层来预测丢失的部分，其意图产生语义上合理但模糊的结果。在本文中，我们为U-Net架构引入了一个特殊的移位连接层，即Shift-Net，用于填充具有尖锐结构和精细纹理的任何形状的缺失区域。为此，已知区域的编码器特征被移位以用作对缺失部分的估计。在解码器特征上引入了引导损耗，以最小化完全连接层之后的解码器特征与缺失部分的地面实况编码器特征之间的距离。利用这种约束，可以使用缺失区域中的解码器特征来指导已知区域中的编码器特征的移位。进一步开发了端到端学习算法来训练Shift-Net。 Paris StreetView和Places数据集上的实验证明了我们的Shift-Net在产生更清晰，细节和视觉上合理的结果方面的效率和有效性。此https URL提供代码和预先训练的模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 25.[Discriminative Region Proposal AdversarialNetworks for High-Quality Image-to-ImageTranslation](http://openaccess.thecvf.com/content_ECCV_2018/papers/Chao_Wang_Discriminative_Region_Proposal_ECCV_2018_paper.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "采用生成对抗网络（GAN），图像到图像的翻译取得了很大进展。然而，对于需要高质量，特别是高分辨率和照片级真实感的翻译任务来说，它仍然非常具有挑战性。在本文中，我们提出了Dis-criminative Region Proposal Adversarial Networks（DRPAN），用于高质量的图像到图像转换。我们将图像到图像转换任务的过程分解为三个迭代步骤，首先是生成具有全局结构的图像但是一些局部工件（通过GAN），其次是使用我们的DRPnet从生成的图像中提出最假的区域第三是在最假的区域实现“图像修复”，通过修改器获得更真实的结果，从而可以逐步优化系统（DRPAN），以更多地关注最神器的本地部分来合成图像。对各种图像到图像翻译任务和数据集的实验验证了我们的方法在人类感知研究和自动量化测量方面都优于现有技术，可以产生高质量的翻译结果。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 26.[PCGAN: Partition-Controlled Human Image Generation](https://arxiv.org/pdf/1811.09928.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "人类图像生成是一项非常具有挑战性的任务，因为它受许多因素的影响。 许多人类图像生成方法专注于生成以给定姿势调节的人类图像，而生成的背景通常是模糊的。 在本文中，我们提出了一种新的Partition-ControlledGAN，用于根据目标姿势和背景生成人体图像。 首先，提取给定图像中的人体姿势，并且分割前景/背景以供进一步使用。 其次，我们提取并融合外观特征，姿势特征和背景特征以生成所需图像。 Market-1501和DeepFash-ion数据集上的实验表明，我们的模型不仅可以生成真实的人类图像，还可以根据需要生成人体姿势和背景。 关于COCO和LIPdatasets的大量实验表明了我们方法的潜力"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
