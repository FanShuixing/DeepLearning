{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Human Pose Estimation\n",
    "\n",
    "### 2D human pose estimation\n",
    "- [Deep High-Resolution Representation Learning for Human Pose Estimation](https://arxiv.org/abs/1902.09212) - [[CODE]](https://github.com/leoxiaobin/deep-high-resolution-net.pytorch) (CVPR 2019)(SOTA)  \n",
    "  - coco\n",
    "  - mpii   \n",
    "  *HRNet作者称HRnet可以用于post tracking，在论文中谈到有在pose tracking2017上训练，可在代码里面没有追踪的代码*\n",
    "- [MultiPoseNet: Fast Multi-Person Pose Estimation using Pose Residual Network](https://arxiv.org/abs/1807.04067) -[[CODE]](https://github.com/salihkaragoz/pose-residual-network-pytorch) (ECCV 2018)\n",
    "    - coco\n",
    "\n",
    "\n",
    "### 3D human pose estimation\n",
    "- [Fast and Robust Multi-Person 3D Pose Estimation from Multiple Views](https://arxiv.org/abs/1901.04111) - [[CODE without train]](https://github.com/zju3dv/mvpose) (CVPR 2019)\n",
    "- [Self-Supervised Learning of 3D Human Pose using Multi-view Geometry](https://arxiv.org/abs/1903.02330) -[[CODE without human3.6m dataset]](https://github.com/mkocabas/EpipolarPose) (CVPR 2019)\n",
    "  - Human3.6m(SOTA)\n",
    "- [ 3D human pose estimation in video with temporal convolutions and semi-supervised training](https://arxiv.org/abs/1811.11742) -[[CODE without human3.6m dataset]](https://github.com/facebookresearch/VideoPose3D)(CVPR 2019)\n",
    "    - Human3.6m\n",
    "- [Learnable Triangulation of Human Pose](https://arxiv.org/abs/1905.05754v1) -[[CODE without release]](https://saic-violet.github.io/learnable-triangulation)(CVPR 2019)\n",
    "    - Human 3.6m\n",
    "    \n",
    "- [Simple Baselines for Human Pose Estimation and Tracking](https://arxiv.org/abs/1804.06208) -[[CODE]](https://github.com/Microsoft/human-pose-estimation.pytorch) (ECCV 2018)  \n",
    "    *获得了Pose Track ECCV2018 challenge winner，但是代码里面并没有release 追踪的代码，跟HRNet是同一作者*\n",
    "- [An Integral Pose Regression System for the ECCV2018 PoseTrack Challenge](https://arxiv.org/abs/1809.06079) -[[CODE]](https://github.com/JimmySuen/integral-human-pose)(ECCV 2018)\n",
    "    - CHALL_H80K(SOTA)\n",
    "    - human3.6m\n",
    "    - mpii\n",
    "    - coco\n",
    " \n",
    "\n",
    "### 6D human pose estimation\n",
    "- [PVNet: Pixel-wise Voting Network for 6DoF Pose Estimation](https://arxiv.org/abs/1812.11788) -[[CODE]](https://github.com/zju3dv/pvnet) (CVPR 2019 oral)\n",
    "- [DenseFusion: 6D Object Pose Estimation by Iterative Dense Fusion](https://arxiv.org/abs/1901.04780) -[[CODE]](https://github.com/j96w/DenseFusion)(CVPR 2019)  \n",
    "  - [YCB video dataset](https://docs.google.com/uc?export=download&id=1if4VoEXNx9W3XCn0Y7Fp15B4GpcYbyYi)\n",
    "\n",
    "### Real-time pose estimation\n",
    "- [Realtime Multi-Person 2D Pose Estimation using Part Affinity Fields](https://arxiv.org/abs/1611.08050) -[[CODE]](https://github.com/ZheC/Realtime_Multi-Person_Pose_Estimation)(CVPR 2017 oral)\n",
    "    - coco\n",
    "\n",
    "\n",
    "Others\n",
    "\n",
    "- OpenPose -[[CODE]](https://github.com/CMU-Perceptual-Computing-Lab/openpose) -[[zhihu]](https://zhuanlan.zhihu.com/p/37526892)\n",
    "    - 基于上述Real-time的工作\n",
    "- AlphaPose -[[CODE]](https://github.com/MVIG-SJTU/AlphaPose#quick-start)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "fr=open('./valid.json')\n",
    "json_info=json.load(fr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'joints_vis': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       " 'joints': [[804.0, 711.0],\n",
       "  [816.0, 510.0],\n",
       "  [908.0, 438.0],\n",
       "  [1040.0, 454.0],\n",
       "  [906.0, 528.0],\n",
       "  [883.0, 707.0],\n",
       "  [974.0, 446.0],\n",
       "  [985.0, 253.0],\n",
       "  [982.7591, 235.9694],\n",
       "  [962.2409, 80.0306],\n",
       "  [869.0, 214.0],\n",
       "  [798.0, 340.0],\n",
       "  [902.0, 253.0],\n",
       "  [1067.0, 253.0],\n",
       "  [1167.0, 353.0],\n",
       "  [1142.0, 478.0]],\n",
       " 'image': '005808361.jpg',\n",
       " 'scale': 4.718488,\n",
       " 'center': [966.0, 340.0]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_info[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
